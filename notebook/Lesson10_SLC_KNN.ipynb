{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbours: Classification\n",
    "\n",
    "<img src='https://onlinecourses.science.psu.edu/stat857/sites/onlinecourses.science.psu.edu.stat857/files/lesson12/image_07.gif'>\n",
    "\n",
    "``k-Nearest Neighbours (KNN)`` falls in the **supervised learning** family of algorithms. Informally, this means that we are given a labelled dataset consiting of training observations $(x,y)$ and would like to capture the relationship between $x$ and $y$. More formally, our goal is to learn a function $h: X \\rightarrow Y$ so that given an unseen observation $x$, $h(x)$ can confidently predict the corresponding output $y$.\n",
    "\n",
    "\n",
    "## Algorithm\n",
    "\n",
    " <img src='../images/knn2.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required modules and load data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fruits = pd.read_table('../data/fruit_data_with_colors.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from fruit label value to fruit name to make results easier to interpret\n",
    "lookup_fruit_name = dict(zip(fruits.fruit_label.unique(), fruits.fruit_name.unique()))   \n",
    "lookup_fruit_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file contains the mass, height, and width of a selection of oranges, lemons and apples. The heights were measured along the core of the fruit. The widths were the widest width perpendicular to the height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting a scatter matrix\n",
    "from matplotlib import cm\n",
    "\n",
    "X = fruits[['height', 'width', 'mass', 'color_score']]\n",
    "y = fruits['fruit_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "cmap = cm.get_cmap('gist_rainbow')\n",
    "scatter = pd.plotting.scatter_matrix(X_train, c= y_train, marker = 'o', s=40, hist_kwds={'bins':15}, figsize=(9,9), cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting a 3D scatter plot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "ax.scatter(X_train['width'], X_train['height'], X_train['color_score'], c = y_train, marker = 'o', s=100)\n",
    "ax.set_xlabel('width')\n",
    "ax.set_ylabel('height')\n",
    "ax.set_zlabel('color_score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, we use the mass, width, and height features of each fruit instance\n",
    "X = fruits[['mass', 'width', 'height']]\n",
    "y = fruits['fruit_label']\n",
    "\n",
    "# default is 75% / 25% train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create classifier object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the classifier (fit the estimator) using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate the accuracy of the classifier on future data, using the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the trained k-NN classifier model to classify new, previously unseen objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first example: a small fruit with mass 20g, width 4.3 cm, height 5.5 cm\n",
    "fruit_prediction = knn.predict([[20, 4.3, 5.5]])\n",
    "lookup_fruit_name[fruit_prediction[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second example: a larger, elongated fruit with mass 100g, width 6.3 cm, height 8.5 cm\n",
    "fruit_prediction = knn.predict([[100, 6.3, 8.5]])\n",
    "lookup_fruit_name[fruit_prediction[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the decision boundaries of the k-NN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adspy_shared_utilities import plot_fruit_knn\n",
    "\n",
    "plot_fruit_knn(X_train, y_train, 1, 'uniform')  # nearest neighbors = 1\n",
    "plot_fruit_knn(X_train, y_train, 5, 'uniform')  # nearest neighbors = 5\n",
    "plot_fruit_knn(X_train, y_train, 10, 'uniform');# nearest neighbors = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How sensitive is k-NN classification accuracy to the choice of the 'k' parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(1,20)\n",
    "scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    scores.append(knn.score(X_test, y_test))\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('accuracy')\n",
    "plt.scatter(k_range, scores)\n",
    "plt.xticks([0,5,10,15,20]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How sensitive is k-NN classification accuracy to the train/test split proportion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2]\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for s in t:\n",
    "\n",
    "    scores = []\n",
    "    for i in range(1,1000):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1-s)\n",
    "        knn.fit(X_train, y_train)\n",
    "        scores.append(knn.score(X_test, y_test))\n",
    "    plt.plot(s, np.mean(scores), 'bo')\n",
    "\n",
    "plt.xlabel('Training set proportion (%)')\n",
    "plt.ylabel('accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons of KNN\n",
    "\n",
    "**Pros**\n",
    "\n",
    "As you can already tell from the previous section, one of the most attractive features of the K-nearest neighbor algorithm is that is simple to understand and easy to implement. With zero to little training time, it can be a useful tool for off-the-bat analysis of some data set you are planning to run more complex algorithms on. Furthermore, KNN works just as easily with multiclass data sets whereas other algorithms are hardcoded for the binary setting. Finally, as we mentioned earlier, the non-parametric nature of KNN gives it an edge in certain settings where the data may be highly “unusual”.\n",
    "\n",
    "**Cons**\n",
    "\n",
    "One of the obvious drawbacks of the KNN algorithm is the computationally expensive testing phase which is impractical in industry settings. Note the rigid dichotomy between KNN and the more sophisticated Neural Network which has a lengthy training phase albeit a very fast testing phase. Furthermore, KNN can suffer from skewed class distributions. For example, if a certain class is very frequent in the training set, it will tend to dominate the majority voting of the new example (large number = more common). Finally, the accuracy of KNN can be severely degraded with high-dimension data because there is little difference between the nearest and farthest neighbor.\n",
    "\n",
    "**Improvements**\n",
    "\n",
    "With that being said, there are many ways in which the KNN algorithm can be improved.\n",
    "\n",
    "- A simple and effective way to remedy skewed class distributions is by implementing **weighed voting**. The class of each of the K neighbors is multiplied by a weight proportional to the inverse of the distance from that point to the given test point. This ensures that nearer neighbors contribute more to the final vote than the more distant ones.\n",
    "- **Changing the distance metric** for different applications may help improve the accuracy of the algorithm. (i.e. Hamming distance for text classification)\n",
    "- **Rescaling your data** makes the distance metric more meaningful. For instance, given 2 features $height$ and $weight$, an observation such as $x=[180,70]$ will clearly skew the distance metric in favor of height. One way of fixing this is by column-wise subtracting the mean and dividing by the standard deviation. Scikit-learn’s normalize() method can come in handy.\n",
    "- **Dimensionality reduction** techniques like PCA should be executed prior to appplying KNN and help make the distance metric more meaningful.\n",
    "- **Approximate Nearest Neighbor** techniques such as using k-d trees to store the training observations can be leveraged to decrease testing time. Note however that these methods tend to perform poorly in high dimensions (20+)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Load the dataset from ``ics.uci balance-scale``\n",
    "\n",
    "Perform the following tasks:\n",
    "- do a bit of feature engineering\n",
    "- create an instance of Neighbours Classifier and fit the data.clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_table('https://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data',\n",
    "                    header=None,\n",
    "                    sep=\",\", \n",
    "                    names=['class','LW','LD','RW','RD'])\n",
    "df1['class'].replace(['L'], 1, inplace=True)\n",
    "df1['class'].replace(['B'], 2, inplace=True)\n",
    "df1['class'].replace(['R'], 3, inplace=True)\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set()\n",
    "sns.pairplot(df1, hue='class', size=1.5);\n",
    "\n",
    "df1.head()\n",
    "\n",
    "X = df1[list(df1.columns)[1:]]\n",
    "y = df1['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, train_size=0.8)\n",
    "\n",
    "knnmodel = KNeighborsClassifier()\n",
    "knnmodel.fit(X_train, y_train)\n",
    "y_predict = knnmodel.predict(X_test)\n",
    "\n",
    "print('Score:', knnmodel.score(X_test, y_test))\n",
    "print ('RMSE:', mean_squared_error(y_predict, y_test) ** 0.5)\n",
    "\n",
    "parameters = [{'weights': ['uniform', 'distance'], 'n_neighbors': [40, 60, 80, 100, 120, 140]}]\n",
    "grid = GridSearchCV(knnmodel, parameters)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.weights)\n",
    "print(grid.best_estimator_.n_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Read the Wine Quality White dataset (``winequality-white.csv``) from your data folder.\n",
    "\n",
    "Perform the follwong tasks:\n",
    "- select features to be the first 11 columns while the last column to be the target\n",
    "- plot the pair plot for all features\n",
    "- separate the data to 80:20 training and testing datasets\n",
    "- create an instance of Neighbours Classifier and fit the data\n",
    "- Measure the accurary and Root Mean Square Error (RMSE) for the fitted model\n",
    "- Perform grid search to find the best parameters for KNN, for ``weights: ['uniform', 'distance']``, ``n_neighbors: [40, 60, 80, 100, 120, 140]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/winequality-white.csv')\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set()\n",
    "sns.pairplot(df, hue='quality', size=1.5);\n",
    "\n",
    "\n",
    "X = df[list(df.columns)[:-1]]\n",
    "y = df['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_predict = model.predict(X_test)\n",
    "\n",
    "print('Score:', model.score(X_test, y_test))\n",
    "print ('RMSE:', mean_squared_error(y_predict, y_test) ** 0.5)\n",
    "\n",
    "parameters = [{'weights': ['uniform', 'distance'], 'n_neighbors': [40, 60, 80, 100, 120, 140]}]\n",
    "grid = GridSearchCV(model, parameters)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.weights)\n",
    "print(grid.best_estimator_.n_neighbors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
