{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "The previous sections outline the fundamental ideas of machine learning, but all of the examples assume that you have numerical data in a tidy, ``[n_samples, n_features]`` format.\n",
    "In the real world, data rarely comes in such a form.\n",
    "With this in mind, one of the more important steps in using machine learning in practice is *feature engineering*: that is, taking whatever information you have about your problem and turning it into numbers that you can use to build your feature matrix.\n",
    "\n",
    "In this section, we will cover a few common examples of feature engineering tasks: features for representing *categorical data*, features for representing *text*, and features for representing *images*.\n",
    "Additionally, we will discuss *derived features* for increasing model complexity and *imputation* of missing data.\n",
    "Often this process is known as *vectorization*, as it involves converting arbitrary data into well-behaved vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Features\n",
    "\n",
    "One common type of non-numerical data is *categorical* data.\n",
    "For example, imagine you are exploring some data on housing prices, and along with numerical features like \"price\" and \"rooms\", you also have \"neighborhood\" information.\n",
    "For example, your data might look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'price': 850000, 'rooms': 4, 'neighborhood': 'Queen Anne'},\n",
    "    {'price': 700000, 'rooms': 3, 'neighborhood': 'Fremont'},\n",
    "    {'price': 650000, 'rooms': 3, 'neighborhood': 'Wallingford'},\n",
    "    {'price': 600000, 'rooms': 2, 'neighborhood': 'Fremont'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be tempted to encode this data with a straightforward numerical mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'Queen Anne': 1, 'Fremont': 2, 'Wallingford': 3};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that this is not generally a useful approach in Scikit-Learn: the package's models make the fundamental assumption that numerical features reflect algebraic quantities.\n",
    "Thus such a mapping would imply, for example, that *Queen Anne < Fremont < Wallingford*, or even that *Wallingford - Queen Anne = Fremont*, which (niche demographic jokes aside) does not make much sense.\n",
    "\n",
    "In this case, one proven technique is to use *one-hot encoding*, which effectively creates extra columns indicating the presence or absence of a category with a value of 1 or 0, respectively.\n",
    "When your data comes as a list of dictionaries, Scikit-Learn's ``DictVectorizer`` will do this for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer(sparse=False, dtype=int)\n",
    "vec.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the 'neighborhood' column has been expanded into three separate columns, representing the three neighborhood labels, and that each row has a 1 in the column associated with its neighborhood.\n",
    "With these categorical features thus encoded, you can proceed as normal with fitting a Scikit-Learn model.\n",
    "\n",
    "To see the meaning of each column, you can inspect the feature names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one clear disadvantage of this approach: if your category has many possible values, this can *greatly* increase the size of your dataset.\n",
    "However, because the encoded data contains mostly zeros, a sparse output can be a very efficient solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer(sparse=True, dtype=int)\n",
    "vec.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.feature_names_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn.preprocessing: OneHotEncoder\n",
    "\n",
    "Encode categorical integer features using a one-hot aka one-of-K scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "titanic = {'name': ['Owen Harris Braund','John Bradley Cuming','Laina Heikkinen'],\n",
    "           'gender':['male', 'male', 'female'],\n",
    "           'ticket':['A/5 21171','PC 17599','STON/O2. 3101282'],\n",
    "           'cabin':['','C85',''],\n",
    "           'embarked':['S','C','S']}\n",
    "\n",
    "df = pd.DataFrame(titanic)\n",
    "enc = preprocessing.LabelEncoder()\n",
    "\n",
    "df['name']=enc.fit_transform(df['name'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "d = defaultdict(preprocessing.LabelEncoder)\n",
    "\n",
    "titanic = {'name': ['Owen Harris Braund','John Bradley Cuming','Laina Heikkinen'],\n",
    "           'gender':['male', 'male', 'female'],\n",
    "           'ticket':['A/5 21171','PC 17599','STON/O2. 3101282'],\n",
    "           'cabin':['','C85',''],\n",
    "           'embarked':['S','C','S']}\n",
    "\n",
    "df = pd.DataFrame(titanic)\n",
    "\n",
    "# Encoding the variable\n",
    "fit = df.apply(lambda x: d[x.name].fit_transform(x))\n",
    "fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse the encoded\n",
    "df_new=fit.apply(lambda x: d[x.name].inverse_transform(x))\n",
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "Pandas has ``get_dummies()`` function that able to convert categorical variables to dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "my_data = np.array([[5, 'a', 1],\n",
    "                    [3, 'b', 3],\n",
    "                    [1, 'b', 2],\n",
    "                    [3, 'a', 1],\n",
    "                    [4, 'b', 2],\n",
    "                    [7, 'c', 1],\n",
    "                    [7, 'c', 1]])                \n",
    "df = pd.DataFrame(data=my_data, columns=['y', 'dummy', 'x'])\n",
    "just_dummy = pd.get_dummies(df['dummy'])\n",
    "df_1 = pd.concat([df, just_dummy], axis=1)      \n",
    "df_1.drop(['dummy', 'c'], inplace=True, axis=1)\n",
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Encode the gender using the one-hot encoding technique.\n",
    "\n",
    "```{python}\n",
    "raw_data = {'first_name': ['Jason', 'Tina', 'Rish', 'Wesley ', 'Amy'],\n",
    "        'last_name': ['Phoon', 'Lau', 'Richer', 'Lee', 'Lim'],\n",
    "        'gender': ['male', 'female', 'male', 'male', 'female']}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "raw_data = {'first_name': ['Jason', 'Tina', 'Rish', 'Wesley ', 'Amy'],\n",
    "        'last_name': ['Phoon', 'Lau', 'Richer', 'Lee', 'Lim'],\n",
    "        'gender': ['male', 'female', 'male', 'male', 'female']}\n",
    "\n",
    "df = pd.DataFrame(raw_data)\n",
    "enc = preprocessing.LabelEncoder()\n",
    "print(df)\n",
    "enc.fit_transform(df['gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many (though not yet all) of the Scikit-Learn estimators accept such sparse inputs when fitting and evaluating models. ``sklearn.preprocessing.OneHotEncoder`` and ``sklearn.feature_extraction.FeatureHasher`` are two additional tools that Scikit-Learn includes to support this type of encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LabelEncoder\n",
    "\n",
    "Encode labels with value between 0 and n_classes-1. LabelEncoder assigns ordinal levels to categorical data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "train = [\"Selangor\", \"Selangor\", \"Wilayah Persekutuan\", \"Kedah\"]\n",
    "test = [\"Wilayah Persekutuan\", \"Wilayah Persekutuan\", \"Selangor\"]\n",
    "\n",
    "le.fit(train)\n",
    "\n",
    "#print the encoded label\n",
    "print(le.classes_)\n",
    "\n",
    "#print the label for Test\n",
    "le.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print back the actual data values\n",
    "le.inverse_transform([1,1,0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Load the iris dataset and use the ``LabelEncoder`` to encode the 'Species' target variabe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "print(iris.target_names)\n",
    "\n",
    "enc = preprocessing.LabelEncoder()\n",
    "\n",
    "enc.fit_transform(iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Features\n",
    "\n",
    "Another common need in feature engineering is to convert text to a set of representative numerical values.\n",
    "For example, most automatic mining of social media data relies on some form of encoding the text as numbers.\n",
    "One of the simplest methods of encoding data is by *word counts*: you take each snippet of text, count the occurrences of each word within it, and put the results in a table.\n",
    "\n",
    "For example, consider the following set of three phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ['problem of evil',\n",
    "          'evil queen',\n",
    "          'horizon problem']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a vectorization of this data based on word count, we could construct a column representing the word \"problem,\" the word \"evil,\" the word \"horizon,\" and so on.\n",
    "While doing this by hand would be possible, the tedium can be avoided by using Scikit-Learn's ``CountVectorizer``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a sparse matrix recording the number of times each word appears; it is easier to inspect if we convert this to a ``DataFrame`` with labeled columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(X.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some issues with this approach, however: the raw word counts lead to features which put too much weight on words that appear very frequently, and this can be sub-optimal in some classification algorithms.\n",
    "One approach to fix this is known as *term frequency-inverse document frequency* (*TF–IDF*) which weights the word counts by a measure of how often they appear in the documents.\n",
    "The syntax for computing these features is similar to the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer()\n",
    "X = vec.fit_transform(sample)\n",
    "pd.DataFrame(X.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Features\n",
    "\n",
    "Another common need is to suitably encode *images* for machine learning analysis.\n",
    "The simplest approach is what we used for the handwritten digits recognition by simply using the pixel values themselves.\n",
    "\n",
    "But depending on the application, such approaches may not be optimal.\n",
    "\n",
    "A comprehensive summary of feature extraction techniques for images is well beyond the scope of this section, but you can find excellent implementations of many of the standard approaches in the [Scikit-Image project](http://scikit-image.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derived Features\n",
    "\n",
    "Another useful type of feature is one that is mathematically derived from some input features.\n",
    "We saw an example of this in **Hyperparameters and Model Validation** when we constructed *polynomial features* from our input data.\n",
    "\n",
    "We saw that we could convert a linear regression into a polynomial regression not by changing the model, but by transforming the input!\n",
    "This is sometimes known as *basis function regression*.\n",
    "\n",
    "For example, this data clearly cannot be well described by a straight line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([4, 2, 1, 3, 7])\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, we can fit a line to the data using ``LinearRegression`` and get the optimal result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X = x[:, np.newaxis]\n",
    "model = LinearRegression().fit(X, y)\n",
    "yfit = model.predict(X)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that we need a more sophisticated model to describe the relationship between $x$ and $y$.\n",
    "\n",
    "One approach to this is to transform the data, adding extra columns of features to drive more flexibility in the model.\n",
    "For example, we can add polynomial features to the data this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X2 = poly.fit_transform(X)\n",
    "print(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derived feature matrix has one column representing $x$, and a second column representing $x^2$, and a third column representing $x^3$.\n",
    "Computing a linear regression on this expanded input gives a much closer fit to our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X2, y)\n",
    "yfit = model.predict(X2)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This idea of improving a model not by changing the model, but by transforming the inputs, is fundamental to many of the more powerful machine learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation of Missing Data\n",
    "\n",
    "Another common need in feature engineering is handling of missing data.\n",
    "We discussed the handling of missing data in ``DataFrame``s and saw that often the ``NaN`` value is used to mark missing values.\n",
    "\n",
    "For example, we might have a dataset that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan\n",
    "X = np.array([[ nan, 0,   3  ],\n",
    "              [ 3,   7,   9  ],\n",
    "              [ 3,   5,   2  ],\n",
    "              [ 4,   nan, 6  ],\n",
    "              [ 8,   8,   1  ]])\n",
    "y = np.array([14, 16, -1,  8, -5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applying a typical machine learning model to such data, we will need to first replace such missing data with some appropriate fill value.\n",
    "This is known as *imputation* of missing values, and strategies range from simple (e.g., replacing missing values with the mean of the column) to sophisticated (e.g., using matrix completion or a robust model to handle such data).\n",
    "\n",
    "The sophisticated approaches tend to be very application-specific, and we won't dive into them here.\n",
    "For a baseline imputation approach, using the mean, median, or most frequent value, Scikit-Learn provides the ``Imputer`` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "X2 = imp.fit_transform(X)\n",
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(strategy='most_frequent')\n",
    "X2 = imp.fit_transform(X)\n",
    "X2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in the resulting data, the two missing values have been replaced with the mean of the remaining values in the column. This imputed data can then be fed directly into, for example, a ``LinearRegression`` estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X2, y)\n",
    "model.predict(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas: fillna\n",
    "\n",
    "Fill NA/NaN values using the specified method. For more information, read the [documentation](http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.fillna.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_data = {'first_name': ['Jason', np.nan, 'Tina', 'Jake', 'Amy'],\n",
    "        'last_name': ['Miller', np.nan, 'Ali', 'Milner', 'Cooze'],\n",
    "        'age': [42, np.nan, 36, 24, 73],\n",
    "        'sex': ['m', np.nan, 'f', 'm', 'f'],\n",
    "        'preTestScore': [4, np.nan, np.nan, 2, 3],\n",
    "        'postTestScore': [25, np.nan, np.nan, 62, 70]}\n",
    "df = pd.DataFrame(raw_data, columns = ['first_name', 'last_name', 'age', 'sex', 'preTestScore', 'postTestScore'])\n",
    "df\n",
    "\n",
    "#Fill in missing data with zeros\n",
    "# df.fillna(0)\n",
    "\n",
    "#Fill in missing data with bfill/ffill\n",
    "# df.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing in preTestScore with the mean value of preTestScore\n",
    "df[\"preTestScore\"].fillna(df[\"preTestScore\"].mean(), inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"postTestScore\"].fillna(df[\"postTestScore\"].median(), inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"age\"].fillna(method='ffill', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sex\"].fillna(method='bfill', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Read the [Heart Disease Data Set](https://archive.ics.uci.edu/ml/datasets/Heart+Disease) for 'processed.cleveland.data' and impute the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "heart = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data', header=None, na_values='?')\n",
    "heart.columns = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','num',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.isnull().values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart[heart.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute the missing values\n",
    "heart[\"ca\"].fillna(heart[\"ca\"].mean(), inplace=True)\n",
    "heart[\"thal\"].fillna(heart[\"thal\"].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nans = lambda heart: heart[heart.isnull().any(axis=1)]\n",
    "nans(heart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.iloc[[87, 166, 192, 266, 287,302]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Pipelines\n",
    "\n",
    "With any of the preceding examples, it can quickly become tedious to do the transformations by hand, especially if you wish to string together multiple steps.\n",
    "For example, we might want a processing pipeline that looks something like this:\n",
    "\n",
    "1. Impute missing values using the mean\n",
    "2. Transform features to quadratic\n",
    "3. Fit a linear regression\n",
    "\n",
    "To streamline this type of processing pipeline, Scikit-Learn provides a ``Pipeline`` object, which can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "model = make_pipeline(SimpleImputer(strategy='mean'),\n",
    "                      PolynomialFeatures(degree=2),\n",
    "                      LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[ nan, 0,   3  ],\n",
    "              [ 3,   7,   9  ],\n",
    "              [ 3,   5,   2  ],\n",
    "              [ 4,   nan, 6  ],\n",
    "              [ 8,   8,   1  ]])\n",
    "y = np.array([14, 16, -1,  8, -5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline looks and acts like a standard Scikit-Learn object, and will apply all the specified steps to any input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)  # X with missing values, from above\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the steps of the model are applied automatically.\n",
    "Notice that for the simplicity of this demonstration, we've applied the model to the data it was trained on; this is why it was able to perfectly predict the result. However, in most time, we need to fine tune the hyperparameters and perform model validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Given the following code, use ``pipeline`` to transform to ``PolynomialFeatures`` with degree=2 and fit the ``LinearRegression`` model.\n",
    "\n",
    "```{python}\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x = np.arange(10)[:, None]\n",
    "y = np.ravel(x) ** 2\n",
    "\n",
    "p = np.array([1, 2])\n",
    "model = LinearRegression().fit(x ** p, y)\n",
    "model.predict(11 ** p)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x = np.arange(10)[:, None]\n",
    "y = np.ravel(x) ** 2 #flatten up the array\n",
    "\n",
    "# p = np.array([1, 2])\n",
    "# model = LinearRegression().fit(x ** p, y)\n",
    "# model.predict(11 ** p)\n",
    "\n",
    "pipemodel = make_pipeline(PolynomialFeatures(degree=2),LinearRegression())\n",
    "\n",
    "pipemodel.fit(x, y)\n",
    "print(y)\n",
    "print(pipemodel.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x = np.arange(10)[:, None]\n",
    "y = np.ravel(x) ** 2\n",
    "\n",
    "p = np.array([1, 2])\n",
    "model = LinearRegression().fit(x ** p, y)\n",
    "model.predict(x ** p)\n",
    "print(y)\n",
    "print(model.predict(x ** p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling and Normalization\n",
    "\n",
    "An alternative approach to Z-score normalization (or standardization) is the so-called Min-Max scaling (often also simply called “normalization” - a common cause for ambiguities).\n",
    "In this approach, the data is scaled to a fixed range - usually 0 to 1.\n",
    "The cost of having this bounded range - in contrast to standardization - is that we will end up with smaller standard deviations, which can suppress the effect of outliers.\n",
    "\n",
    "A Min-Max scaling is typically done via the following equation:\n",
    "\n",
    "$$X_{norm}=\\frac{X-X_{min}}{X_{max} - X_{min}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../data/wine_data.csv',header=None,usecols=[0,1,2])\n",
    "df.columns=['Class label', 'Alcohol', 'Malic acid']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "#standard scale transform\n",
    "std_scale = preprocessing.StandardScaler().fit(df[['Alcohol', 'Malic acid']])\n",
    "df_std = std_scale.transform(df[['Alcohol', 'Malic acid']])\n",
    "\n",
    "#min max scale transform\n",
    "minmax_scale = preprocessing.MinMaxScaler().fit(df[['Alcohol', 'Malic acid']])\n",
    "df_minmax = minmax_scale.transform(df[['Alcohol', 'Malic acid']])\n",
    "\n",
    "print('Mean after standardization:\\nAlcohol={:.2f}, Malic acid={:.2f}'\n",
    "      .format(df_std[:,0].mean(), df_std[:,1].mean()))\n",
    "print('\\nStandard deviation after standardization:\\nAlcohol={:.2f}, Malic acid={:.2f}'\n",
    "      .format(df_std[:,0].std(), df_std[:,1].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot to show the standardized and normalized data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot():\n",
    "    plt.figure(figsize=(8,6))\n",
    "\n",
    "    plt.scatter(df['Alcohol'], df['Malic acid'],color='green', label='input scale', alpha=0.5)\n",
    "    plt.scatter(df_std[:,0], df_std[:,1], color='red', label='Standardized N, mu=0, sigma=1', alpha=0.3)\n",
    "    plt.scatter(df_minmax[:,0], df_minmax[:,1],color='blue', label='min-max scaled [min=0, max=1]', alpha=0.3)\n",
    "\n",
    "    plt.title('Alcohol and Malic Acid content of the wine dataset')\n",
    "    plt.xlabel('Alcohol')\n",
    "    plt.ylabel('Malic Acid')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Use the ``MinMaxScaler`` to normalized the following dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "s1 = pd.Series([1, 2, 3, 4, 5, 6], index=(range(6)))\n",
    "s2 = pd.Series([10, 9, 8, 7, 6, 5], index=(range(6)))\n",
    "df = pd.DataFrame(dict(s1=s1,s2=s2), columns=['s1','s2'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#min max scale transform\n",
    "minmax_scale = MinMaxScaler().fit(df[['s1','s2']])\n",
    "df_minmax = minmax_scale.transform(df[['s1', 's2']])\n",
    "\n",
    "print(df_minmax)\n",
    "\n",
    "def plot():\n",
    "    plt.figure(figsize=(8,6))\n",
    "\n",
    "    plt.scatter(df['s1'], df['s2'],color='green', label='input scale', alpha=0.5)\n",
    "    plt.scatter(df_minmax[:,0], df_minmax[:,1],color='blue', label='min-max scaled [min=0, max=1]', alpha=0.3)\n",
    "\n",
    "    plt.title('s1 and s2 dataset')\n",
    "    plt.xlabel('s1')\n",
    "    plt.ylabel('s2')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot()\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
