{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "\n",
    "\n",
    "Linear regression models are a good starting point for regression tasks. Such models are popular because they can be fit very quickly, and are very interpretable. You are probably familiar with the simplest form of a linear regression model (i.e., fitting a straight line to data) but such models can be extended to model more complicated data behavior.\n",
    "\n",
    "<img src =\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/400px-Linear_regression.svg.png\">\n",
    "\n",
    "In this section, we will start with a quick intuitive walk-through of the mathematics behind this well-known problem, before seeing how before moving on to see how linear models can be generalized to account for more complicated patterns in data.\n",
    "\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression\n",
    "\n",
    "We will start with the most familiar linear regression, a straight-line fit to data. A straight-line fit is a model of the form\n",
    "\n",
    "$$y=\\beta_0+\\beta_1x$$\n",
    " \n",
    "where $\\beta_1$ is commonly known as the slope, and $\\beta_0$ is commonly known as the intercept.\n",
    "Consider the following data, which is scattered about a line with a slope of 2 and an intercept of -5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(50)\n",
    "y = 2 * x - 5 + rng.randn(50)\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Scikit-Learn's ``LinearRegression`` estimator to fit this data and construct the best-fit line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "model.fit(x[:, np.newaxis], y)\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slope and intercept of the data are contained in the model's fit parameters, which in Scikit-Learn are always marked by a trailing underscore. Here the relevant parameters are ``coef_`` and ``intercept_``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model slope:    \", model.coef_[0])\n",
    "print(\"Model intercept:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the results are very close to the inputs, as we might hope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict new value\n",
    "\n",
    "Using ``predict`` method, we can estimate the new response value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict for a new observation\n",
    "X_new = np.array([[2.5]])\n",
    "model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statmodels for Linear Regression\n",
    "\n",
    "Linear models with independently and identically distributed errors, and for errors with heteroscedasticity or autocorrelation. This module allows estimation by ordinary least squares (OLS), weighted least squares (WLS), generalized least squares (GLS), and feasible generalized least squares with autocorrelated AR(p) errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "# from pandas.core import datetools\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(50)\n",
    "y = 2 * x - 5 + rng.randn(50)\n",
    "plt.scatter(x,y);\n",
    "\n",
    "# Note the difference in argument order\n",
    "model = sm.OLS(y, x).fit()\n",
    "predictions = model.predict(x) # make the predictions by the model\n",
    "\n",
    "# Print out the statistics\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``LinearRegression`` estimator is much more capable than this, however—in addition to simple straight-line fits, it can also handle multidimensional linear models of the form\n",
    "\n",
    "$$y=\\beta_0+\\beta_1x_1+\\beta_2x_2+⋯+\\beta_nx_n$$\n",
    "\n",
    "where there are multiple $x$ values. Geometrically, this is akin to fitting a plane to points in three dimensions, or fitting a hyper-plane to points in higher dimensions.\n",
    "\n",
    "The multidimensional nature of such regressions makes them more difficult to visualize, but we can see one of these fits in action by building some example data, using NumPy's matrix multiplication operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = 10 * rng.rand(100, 3)\n",
    "y = 0.5 + np.dot(X, [1.5, -2., 1.])\n",
    "\n",
    "model.fit(X, y)\n",
    "print(model.intercept_)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the $y$ data is constructed from three random $x$ values, and the linear regression recovers the coefficients used to construct the data.\n",
    "\n",
    "In this way, we can use the single ``LinearRegression`` estimator to fit lines, planes, or hyperplanes to our data. It still appears that this approach would be limited to strictly linear relationships between variables, but it turns out we can relax this as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basis Function Regression\n",
    "\n",
    "One trick you can use to adapt linear regression to nonlinear relationships between variables is to transform the data according to basis functions. The idea is to take our multidimensional linear model:\n",
    "\n",
    "$$y=\\beta_0+\\beta_1x_1+\\beta_2x_2+⋯+\\beta_nx_n$$\n",
    " \n",
    "and build the $x_1$, $x_2$, $x_3$ and so on, from our single-dimensional input $x$. That is, we let  $x_n=f_n(x)$, where $f_n()$ is some function that transforms our data.\n",
    "\n",
    "For example, if  $f_n(x)=x_n$ , our model becomes a polynomial regression:\n",
    "\n",
    "$$y=\\beta_0+\\beta_1x_1+\\beta_2x^2+\\beta_3x^3⋯+\\beta_nx^n$$\n",
    " \n",
    "Notice that this is still a linear model—the linearity refers to the fact that the coefficients $a_n$ never multiply or divide each other. What we have effectively done is taken our one-dimensional $x$ values and projected them into a higher dimension, so that a linear fit can fit more complicated relationships between $x$ and $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial basis functions\n",
    "\n",
    "This polynomial projection is useful enough that it is built into Scikit-Learn, using the ``PolynomialFeatures`` transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "x = np.array([2, 3, 4])\n",
    "poly = PolynomialFeatures(3, include_bias=False)\n",
    "poly.fit_transform(x[:, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that the transformer has converted our one-dimensional array into a three-dimensional array by taking the exponent of each value. This new, higher-dimensional data representation can then be plugged into a linear regression.\n",
    "\n",
    "By using the scikit-learn's [``pipeline``](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), we can make a 7th-degree polynomial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "poly_model = make_pipeline(PolynomialFeatures(7),\n",
    "                           LinearRegression()) #more degree will cause to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this transform in place, we can use the linear model to fit much more complicated relationships between $x$ and $y$. For example, here is a sine wave with noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(50)\n",
    "y = np.sin(x) + 0.1 * rng.randn(50)\n",
    "\n",
    "poly_model.fit(x[:, np.newaxis], y)\n",
    "yfit = poly_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our linear model, through the use of 7th-order polynomial basis functions, can provide an excellent fit to this non-linear data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Read the csv data from http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv where\n",
    "\n",
    "The features are:\n",
    "- TV: advertising dollars spent on TV for a single product in a given market (in thousands of dollars)\n",
    "- Radio: advertising dollars spent on Radio\n",
    "- Newspaper: advertising dollars spent on Newspaper\n",
    "\n",
    "and the response is:\n",
    "- Sales: sales of a single product in a given market (in thousands of widgets)\n",
    "\n",
    "Explore the data and find out the answers for the following questions:\n",
    "- Is there a relationship between ads and sales?\n",
    "- How strong is that relationship?\n",
    "- Which ad types contribute to sales?\n",
    "- What is the effect of each ad type of sales?\n",
    "- Given ad spending in a particular market, can sales be predicted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data into a DataFrame\n",
    "import pandas as pd\n",
    "data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "Develop a simple LinearRegression for each feature i.e. TV, Radio, Newspaper against Sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TV vs Sales\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# TVmodel = LinearRegression(fit_intercept=True)\n",
    "\n",
    "# Xfit = TVmodel.fit(data.TV[:,np.newaxis], data.Sales)\n",
    "\n",
    "# yfit = TVmodel.predict(Xfit)\n",
    "\n",
    "# plt.scatter(data.TV, data.Sales)\n",
    "# plt.plot(Xfit, yfit, color='red');\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "TVmodel = LinearRegression(fit_intercept=True)\n",
    "TVmodel.fit(data.TV[:,np.newaxis],data.sales)\n",
    "TVfit = TVmodel.predict(data.TV[:,np.newaxis])\n",
    "\n",
    "plt.scatter(data.TV, data.sales)\n",
    "plt.plot(data.TV, TVfit, color='green')\n",
    "plt.xlabel('TV advertisement')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('TV vs Sales')\n",
    "\n",
    "print(TVmodel.coef_)\n",
    "print(TVmodel.intercept_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Radio vs Sales\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "Radiomodel = LinearRegression(fit_intercept=True)\n",
    "Radiomodel.fit(data.radio[:,np.newaxis],data.sales)\n",
    "Radiofit = Radiomodel.predict(data.radio[:,np.newaxis])\n",
    "\n",
    "plt.scatter(data.radio, data.sales)\n",
    "plt.plot(data.radio, Radiofit, color='green')\n",
    "plt.xlabel('Radio advertisement')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Radio vs Sales')\n",
    "\n",
    "print(Radiomodel.coef_)\n",
    "print(Radiomodel.intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Newspaper vs Sales\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "Newspapermodel = LinearRegression(fit_intercept=True)\n",
    "Newspapermodel.fit(data.newspaper[:,np.newaxis],data.sales)\n",
    "Newspaperfit = Newspapermodel.predict(data.newspaper[:,np.newaxis])\n",
    "\n",
    "plt.scatter(data.newspaper, data.sales)\n",
    "plt.plot(data.newspaper, Newspaperfit, color='green')\n",
    "plt.xlabel('Newspaper advertisement')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Newspaper vs Sales')\n",
    "\n",
    "print(Newspapermodel.coef_)\n",
    "print(Newspapermodel.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression\n",
    "\n",
    "Simple linear regression can easily be extended to include multiple features. This is called multiple linear regression:\n",
    "\n",
    "$$ y=\\beta_0+\\beta_1x_1+...+\\beta_nx_n$$\n",
    "\n",
    "Each $x$ represents a different feature, and each feature has its own coefficient. In this case:\n",
    "\n",
    "$$y=\\beta_0+\\beta_1×TV+\\beta_2×Radio+\\beta_3×Newspaper$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multiple Linear Regression\n",
    "from sklearn import linear_model\n",
    "\n",
    "LR = linear_model.LinearRegression(fit_intercept=True)\n",
    "LR.fit(data.loc[:,['TV','radio','newspaper']], data.sales)\n",
    "\n",
    "print('Model coefficients:',LR.coef_)\n",
    "print('Model intercept:',LR.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the three features vs Sales and also the fitted line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(12.5, 7.5)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(xs=data['TV'], ys=data['sales'], zs=data['newspaper'], c=('blue'))\n",
    "\n",
    "ax.set_ylabel('Sales'); ax.set_xlabel('TV'); ax.set_zlabel('Newspaper')\n",
    "ax.view_init(10, -45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict New Observation\n",
    "\n",
    "Predict the Sales for ddvertisement spending TV=100, Radio=25 and Newspaper=25 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction\n",
    "X_new = np.array([[100,25,25]])\n",
    "LR.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Load the sklearn dataset of the [Boston house prices](https://archive.ics.uci.edu/ml/datasets/housing) (link to the description). The Boston house prices is a dataset designated for testing and learning machine learning tools, it comes with a description of the dataset. \n",
    "\n",
    "Answer the following questions:\n",
    "- Describe the summary of the data\n",
    "- Perform multiple linear regression and determine:\n",
    "    - The $R^2$ value\n",
    "    - The coefficients and intercept values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports datasets from scikit-learn\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "# loads Boston dataset from datasets library \n",
    "data = datasets.load_boston() \n",
    "\n",
    "# description of the Boston dataset\n",
    "print (data.DESCR)\n",
    "\n",
    "# check the data type of data\n",
    "\n",
    "print(type(data))\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data/predictors as the pre-set feature names  \n",
    "data.feature_names\n",
    "\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "X\n",
    "# Put the target (housing value -- MEDV) in another DataFrame\n",
    "\n",
    "y = pd.DataFrame(data.target, columns=['MEDV'])\n",
    "y\n",
    "print(y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculate the R2 value\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from pandas.core import datetools\n",
    "\n",
    "# Note the difference in argument order\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Print out the statistics\n",
    "# model.rsquared\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficients and intercept\n",
    "# from sklearn import linear_model\n",
    "\n",
    "house = linear_model.LinearRegression(fit_intercept=True)\n",
    "house.fit(X, y)\n",
    "\n",
    "print('Model coefficients:',house.coef_)\n",
    "print('Model intercept:',house.intercept_)\n",
    "\n",
    "# plt.scatter(X,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
